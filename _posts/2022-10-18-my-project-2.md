---
layout: post
title:  Web Scraping S&P 500 Data Using Pandas
date:   2022-10-18
author: Max Bucanov
description: Pandas is an easy way to web scrape the data that you are interested in from a website and then use that data to perform Explaratory Data Analysis. This is how I used this package to get data about 500 US companies from a website.
image: /assets/images/Web Scraping.jpeg
---


I have always been interested in the stock market and how our small, consistent investments now can pay off in the future, providing us a comfortable retirement. In fact, one of the reasons why I chose to learn Data Science and Machine learning is because I wanted to use the historical stock market data to predict their potential returns in the future.

Today, data scientists can use models to predict how the market may behave short term and/or long term. This is done by exploring data from past stock market behaviors and using that information to forecast what might happen in the future.

There are many ways how we can import data into Jupyter Notebook to analyze the dataset. We can read in a CSV file, or a text file, for example. The only downside with a static file is that it may not work for short term forecast. As we know, stock market can be very volatile, and the prices fluctuate every day. This is why getting the data directly from a website where prices get updated daily could be a better solution. This process in Data Science is called Web Scraping.

<p align="center" >
   <img src= "https://raw.githubusercontent.com/maxbucanov/stat386-projects/main/assets/images/what_is_web_scraping.png" alt="" style="width:600px;"/>
</p>

Web scraping is a great way to get data directly from websites. However, when extracting data, it's important to make sure that we only extract publicly available information. Some data may be protected by regulations, so be very careful and don't scrape personal or confidential data. One of the best ways to check if a website allows scraping is to use the code below:


   <p align="left" >
   import requests 
   </p>
   <p align="left" >
   from bs4 import BeautifulSoup
   </p>
   <p align="left" >
   r=requests.get(" ENTER YOUR URL")
   </p>
   <p align="left" >
   r.status_code 
   </p>
   

   
   If the status code is 200, then you are allowed to scrape the website. If the status code is anything other than 200, then you cannot scrape the website, or are allowed to scrape it partially.
   
   Since the data about stock market is a public data, I knew that I shouldn't be having any issues scraping it. However, to be sure, I still used **requests** library to see if the website I selected allowed web scraping. After running the code, the output was 200.
   
   
<p align="center" >
   <img src= "https://raw.githubusercontent.com/maxbucanov/stat386-projects/main/assets/images/Libraries.png" alt="" style="width:500px;"/>
</p>

To scrape the website I chose to go with Pandas using _pd.read_html()_. This is a very easy and fast way to extract data from a website.
I used _len()_ function to check how many tables there are on the website and _type()_ function to check the type of the table.

<p align="center" >
   <img src= "https://raw.githubusercontent.com/maxbucanov/stat386-projects/main/assets/images/Data_type.png" alt="" style="width:500px;"/>
</p>

Then I simply read in the dataframe. Since there is only one table on the website I used the dfs[0] command. (0 refers to the first table, 1 second, 2 third and so on). Since there is only one table I selected dfs[0]. When I print the dataframe, I always prefer using the head() function that will show the first 5 observations.

<p align="center" >
   <img src= "https://raw.githubusercontent.com/maxbucanov/stat386-projects/main/assets/images/DataFrame.png" alt="" style="width:500px;"/>
</p>

### Conclusion
Web Scraping can be a very powerful tool in for a Data Scientist, especially if you don't want to work with a static file and need to scrape data that keeps updating on the website.

There are multiple ways to extract data from a website. Packages such as pandas, beautifulsoup or data API are a great way to scrape the website.

Although Web Scraping isn't illegal, you need to make sure you adhere to the website rules and only scrape public data. No confidential data, no personal data; only publicly available information. When using requests library, run the _url = request.get()_ and _r.status_code_ to see if you are allowed to webscrape.

Make sure you know what the following status codes mean:

- 200 (The website allows you to scrape the data")
- 403 (Forbidden - request was valid but the server is refusing
action)
- 404 (Not found)


To check data and the scraping process click on [this Repository](https://github.com/maxbucanov/Web-Scraping-Blog)



